# -*- coding: utf-8 -*-
"""Elias Final Copy of NBABuild.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H8trT2Ygxxq4h3JCNVeWwSh6vxED9Kvc

# Read Data
"""

import pandas as pd

nba2kFile = '/content/nba2kStats.csv'
nba2k = pd.read_csv(nba2kFile)

nbaGameFile = '/content/nbaGameLogs.csv'
nbaGames = pd.read_csv(nbaGameFile)

# Sort the GameLog dataset by date and reset the index
nbaGames = nbaGames.sort_values("date")
nbaGames = nbaGames.reset_index(drop=True)

"""# Data Exploratory Analysis

"""

import seaborn as sns
import matplotlib.pyplot as plt


pairplot = sns.pairplot(nba2k[['PTS','AST','REB','MIN','rankings','+/-']], height=2)
pairplot.fig.suptitle('Pair Plot of Key Player Statistics', y=1.02)
plt.show()

boxplot = sns.boxplot(x="TEAM", y='rankings', data = nba2k)
boxplot.set_title("Boxplot of Player Rankings By Team")
boxplot.set_xticklabels(boxplot.get_xticklabels(), rotation=90)
plt.show()

plt.show()

"""# Data Pre-Processing and Feature Engineering

To begin, we group the nbaGames data by team, and compute a rolling average for several features including the average offensive, defensive score for up to the last 10 most recent games. If less than 10 games have been played in the current season, we still take the average of the previous games. Furthermore, we developed a LastTenGames column that measures how many games each team has won. This measures a "hot" or "cold" streak for both teams.
"""

# Perform group by and averaging transformations for features
nbaGames["Avg_ortg"] = nbaGames.groupby("team")['ortg'].transform(lambda x: x.shift(1).rolling(10, min_periods=1).mean())
nbaGames["Avg_drtg"] = nbaGames.groupby("team")['drtg'].transform(lambda x: x.shift(1).rolling(10, min_periods=1).mean())
nbaGames["Avg_ortg_opp"] = nbaGames.groupby("team_opp")['ortg_opp'].transform(lambda x: x.shift(1).rolling(10, min_periods=1).mean())
nbaGames["Avg_drtg_opp"] = nbaGames.groupby("team_opp")['drtg_opp'].transform(lambda x: x.shift(1).rolling(10, min_periods=1).mean())
nbaGames['LastTenGames'] = nbaGames.groupby('team')['won'].apply(lambda x: x.shift(1).rolling(10, min_periods=1).sum())
nbaGames['LastTenGames_opp'] = nbaGames.groupby('team_opp')['won'].apply(lambda x: x.shift(1).rolling(10, min_periods=1).count() - x.shift(1).rolling(10, min_periods=1).sum())

data = nbaGames[['team','team_opp','home','season','date','won',"Avg_ortg","Avg_drtg","Avg_ortg_opp","Avg_drtg_opp",'LastTenGames','LastTenGames_opp']]
data['season'] = data['season'].astype(str)
data.head(5)

"""Furthermore, we utilize the nba2k dataset to sort the values by season, team, and minutes played for each player. Afterwards, we had to ensure that the "SEASON" column matched with the"""

# Initial sorting and data manipulation
nba2k = nba2k.sort_values(by=['SEASON', 'TEAM', 'MIN'], ascending=[True, True, False])
nba2k["SEASON"] = '20' + nba2k['SEASON'].astype(str).str[-2:]
nba2k['rank'] = nba2k.groupby(['TEAM', 'SEASON']).cumcount() + 1

# Calculate the top 10 players for each team in each season based on minutes played
top_10 = nba2k.groupby(['SEASON', 'TEAM']).apply(lambda group: group.nlargest(10, 'MIN')).reset_index(drop=True)

# Compute additional features from top_10 DataFrame
# Average ranking of the top 10 players
avg_rank = top_10.groupby(['SEASON', 'TEAM'])['rankings'].mean().reset_index(name='AvgRatings')

# Injury score (Availability Score)
injury = top_10.groupby(['SEASON', 'TEAM'])['GP'].sum().reset_index(name='GP')
injury['AvailabilityScore'] = injury['GP'] / top_10.groupby(['SEASON', 'TEAM'])['GP'].max().reset_index(name='MaxGP')['MaxGP']

# Maximum ranking of the top 10 players
max_ranking = top_10.groupby(['SEASON', 'TEAM'])['rankings'].max().reset_index(name='MaxRatingPlayer')

# Calculate the average plus/minus for the top 10 players
avg_plus_minus = top_10.groupby(['SEASON', 'TEAM'])['+/-'].mean().reset_index(name='AvgPlusMinusOfTop10Players')

# Calculate the average age for the top 10 players
avg_age = top_10.groupby(['SEASON', 'TEAM'])['AGE'].mean().reset_index(name='AverageAge')

# Create a binary flag for whether each player is 90+ rated, using the correct column name for ratings
top_10['is_90plus_rated'] = (top_10['rankings'] >= 90).astype(int)

# Count the number of 90+ rated players
num_90plus_players = top_10.groupby(['SEASON', 'TEAM'])['is_90plus_rated'].sum().reset_index(name='GenerationSuperstars')

# Create a new DataFrame from nba2k to ensure 'SEASON' and 'TEAM' columns are present
newData = nba2k[['SEASON', 'TEAM']].drop_duplicates()

# Merge all the new features with the newData DataFrame
newData = pd.merge(newData, avg_rank, on=['SEASON', 'TEAM'], how='left')
newData = pd.merge(newData, injury[['SEASON', 'TEAM', 'AvailabilityScore']], on=['SEASON', 'TEAM'], how='left')
newData = pd.merge(newData, max_ranking, on=['SEASON', 'TEAM'], how='left')
newData = pd.merge(newData, avg_plus_minus, on=['SEASON', 'TEAM'], how='left')
newData = pd.merge(newData, avg_age, on=['SEASON', 'TEAM'], how='left')
newData = pd.merge(newData, num_90plus_players, on=['SEASON', 'TEAM'], how='left')

# Final selection and renaming of columns
newData['team'] = newData['TEAM']
newData['season'] = newData['SEASON']

# Drop the columns that are not needed
columns_to_drop = ['TEAM', 'SEASON']
newData.drop(columns=columns_to_drop, inplace=True)

# Display the DataFrame
newData.head()

actualData = pd.merge(data,newData, on=['season','team'], how ='left')
actualData = actualData.dropna()

# Now we add the opponent's AvgRatings and MaxRatingPlayer and AvailabilityScore
newData['team_opp'] = newData['team']
newData['AvailabilityScore_opp'] = newData['AvailabilityScore']
newData['AvgRatings_opp'] = newData['AvgRatings']
newData['MaxRatingPlayer_opp'] = newData['MaxRatingPlayer']
newData['AvgPlusMinusOfTop10Players_opp'] = newData['AvgPlusMinusOfTop10Players']
newData['AverageAge_opp'] = newData['AverageAge']
newData['GenerationSuperstars_opp'] = newData['GenerationSuperstars']
del newData['team']
del newData['AvailabilityScore']
del newData['AvgRatings']
del newData['MaxRatingPlayer']
del newData['AvgPlusMinusOfTop10Players']
del newData['AverageAge']
del newData['GenerationSuperstars']


newData
actualData = pd.merge(actualData,newData, on=['season','team_opp'], how ='left')
actualData = actualData.dropna()

actualData

"""# Initial Logistic Regression Classifier as Baseline for Improvement"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
modelData = actualData[actualData["season"].isin(['2016'])]
model = LogisticRegression()
x = modelData
x = x.drop(['season','date','won'],axis=1)
y = modelData['won']
label = LabelEncoder()

x['team'] = label.fit_transform(x['team'])
x['team_opp'] = label.fit_transform(x['team_opp'])

X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)
model.fit(X_train,y_train)
pred = model.predict(X_test)
report = classification_report(y_test,pred)
print(report)

# Predict probabilities for the positive class (class 1)
y_prob = model.predict_proba(X_test)[:, 1]

# Compute ROC curve and ROC area for each class
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Baseline Logistic Regression ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# Initial Random Forest Classifier as Baseline for Improvement

The below RandomForestClassifier is initialized without grid search / hyperparameter tuning to demonstrate an initial baseline for our accuracy score. We will see how through utilizing different classifiers, including stacking / ensemble methods as well as utilizing gridsearch for hyperparameter tuning will improve our classifiers from this initial baseline measurement in subsequent cells.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rfc = RandomForestClassifier(n_estimators=1000,random_state=42)

rfc.fit(X_train,y_train)

pred = rfc.predict(X_test)

accuracy = accuracy_score(y_test, pred)
print("Accuracy for baseline Random Forest:")
print(accuracy)
report = classification_report(y_test,pred)
print(report)

# Predict probabilities for the positive class (class 1)
y_prob = rfc.predict_proba(X_test)[:, 1]

# Compute ROC curve and ROC area for each class
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Baseline Random Forest ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# SVC Classifier"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold

# Scale X data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create a SVC Classifier using constant random_state for reproducability
svc = SVC(random_state=42, probability=True) #probability=True (enables Ensemble to using 'soft' voting)

param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'degree': [2, 3, 4],
}

# Perform GridSearch using the above param grid, scoring based on highest accuracy
grid_search_svc = GridSearchCV(svc, param_grid, cv=10, scoring='accuracy', n_jobs=-1)

grid_search_svc.fit(X_train_scaled, y_train)

best_params_rus = grid_search_svc.best_params_

best_svc_model = grid_search_svc.best_estimator_.fit(X_train_scaled, y_train)

y_pred_svc = best_svc_model.predict(X_test_scaled)

# Calculate metrics and print out report
report = classification_report(y_test,y_pred_svc)
accuracy = accuracy_score(y_test, y_pred_svc)
print("Accuracy for SVC Classifier:")

print(accuracy)
print(report)

# Predict probabilities for the positive class (class 1)
y_prob = svc.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and ROC area for each class
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

y_prob = best_svc_model.predict_proba(X_test_scaled)[:, 1]
# Compute ROC curve and ROC area for each class
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('SVC ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# Random Forest Classifier"""

from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold


# Create a Random Forest classifier
rf = RandomForestClassifier()

# Define the hyperparameter grid for RF classifier
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 8, 12],
    'max_features': [6, 9, 12, 15]
}

# Perform GridSearch using the above param grid, scoring based on highest accuracy
grid_search_rf = GridSearchCV(rf, param_grid=param_grid_rf, cv=10, scoring='accuracy', n_jobs=-1)

grid_search_rf.fit(X_train_scaled, y_train)

best_params_rus = grid_search_rf.best_params_

best_rf_model = grid_search_rf.best_estimator_.fit(X_train_scaled, y_train)

y_pred_rf = best_rf_model.predict(X_test_scaled)

# Calculate metrics and print out report
report_rf = classification_report(y_test, y_pred_rf)
accuracy = accuracy_score(y_test, y_pred_rf)
print("Accuracy for Random Forest Classifier:")
print(accuracy)
print(report_rf)

# Predict probabilities for the positive class (class 1)
y_prob_rf = best_rf_model.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and ROC area for each class
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Plot ROC curve for Random Forest
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Random Forest ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# XGBoost Classifier"""

from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE


xgb = XGBClassifier(random_state=42)

# Define a parameter grid for grid search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.005, 0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
}

# Initialize a new instance of SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Perform GridSearch using the above param grid, scoring based on highest accuracy
grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)

# Fit the model to the resampled training data
grid_search_xgb.fit(X_train_resampled, y_train_resampled)

# Get the best parameters and model for XGB
best_params = grid_search_xgb.best_params_
best_xgb_model = grid_search_xgb.best_estimator_

# Predictions on the test set
y_pred = best_xgb_model.predict(X_test_scaled)

# Calculate metrics and print out report
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
print("Accuracy for XGBoost Classifier")
print(accuracy)
print(classification_rep)

# Predict probabilities for the positive class (class 1)
y_prob = best_xgb_model.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and ROC area for each class
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('XGBoost ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# Ensemble Classifier"""

from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline

# Create a VotingClassifier
ensemble_classifier = VotingClassifier(
    estimators=[
        ('rf', rf),
        ('svc', svc),
        ('xgb', xgb)
    ],
    voting='hard'
)

# Create a pipeline with the ensemble model
pipeline = Pipeline([
    ('ensemble', ensemble_classifier)
])

# Define the parameter grid for grid search for Ensemble
param_grid = {
    'ensemble__svc__C': [0.1, 1, 10],
    'ensemble__rf__n_estimators': [50, 100, 200],
    'ensemble__xgb__max_depth': [3, 5, 7]
}

# Perform GridSearch using the above param grid, scoring based on highest accuracy
grid_search_ens = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')

# Fit the grid search to the data
grid_search_ens.fit(X_train_scaled, y_train)

# Get the best params
best_params_ensemble = grid_search_ens.best_params_

# Make predictions on the test set using the best estimator
y_pred_ensemble = grid_search_ens.best_estimator_.predict(X_test_scaled)

# Calculate metrics and print out report
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy Ensemble:")
print(accuracy)

print("\nClassification Report for Ensemble:")
print(classification_report(y_test, y_pred_ensemble))

# Predict probabilities for the positive class (class 1)
y_prob_ensemble = grid_search_ens.best_estimator_.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and ROC area for each class
fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, y_prob_ensemble)
roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)

# Plot ROC curve for the ensemble
plt.figure(figsize=(8, 6))
plt.plot(fpr_ensemble, tpr_ensemble, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_ensemble:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Ensemble ROC Curve')
plt.legend(loc='lower right')
plt.show()

print("\nClassification Report for Ensemble:")
print(classification_report(y_test, y_pred_ensemble))

"""# Stacking Classifier"""

from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Create base models (XGB already defined from above)
grad_boost = GradientBoostingClassifier(random_state=42)
ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)

# Define hyperparameter grids for AdaBoost and GradientBoost
param_grid_adaboost = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.001, 0.01, 0.1, 1.0]
}

param_grid_gradientboost = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.001, 0.01, 0.1, 1.0],
    'max_depth': [3, 5, 7,  10]
}

param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
}

# Create stacking ensemble model with grid search for each base model
stacking_model = StackingClassifier(
    estimators=[('gb', grad_boost), ('adaboost', ada_boost), ('xgb', xgb)],
    final_estimator=RandomForestClassifier(random_state=42)
)

# Define the parameter grid for grid search on the stacking model
param_grid_stacking = {
    'final_estimator__n_estimators': [50, 100, 200],
    'final_estimator__max_depth': [3, 5, 7]
}

# Perform grid search for AdaBoost
grid_search_adaboost = GridSearchCV(ada_boost, param_grid_adaboost, cv=5, scoring='accuracy')
grid_search_adaboost.fit(X_train_scaled, y_train)

# Perform grid search for GradientBoost
grid_search_gradientboost = GridSearchCV(grad_boost, param_grid_gradientboost, cv=5, scoring='accuracy')
grid_search_gradientboost.fit(X_train_scaled, y_train)

# Perform grid search for XGBoost
grid_search_xgboost = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy')
grid_search_xgboost.fit(X_train_scaled, y_train)

# Create a separate RandomForestClassifier for the final estimator
final_estimator_rf = RandomForestClassifier(random_state=42)

# Perform grid search for the stacking model
grid_search_stacking = GridSearchCV(stacking_model, param_grid_stacking, cv=5, scoring='accuracy')
grid_search_stacking.fit(X_train_scaled, y_train)

# Get the best hyperparameters for each model
best_params_adaboost = grid_search_adaboost.best_params_
best_params_gradientboost = grid_search_gradientboost.best_params_
best_params_xgboost = grid_search_xgboost.best_params_
best_params_stacking = grid_search_stacking.best_params_

# Create the final stacking model with the best hyperparameters
final_stacking_model = StackingClassifier(
    estimators=[
        ('gb', GradientBoostingClassifier(**best_params_gradientboost, random_state=42)),
        ('adaboost', AdaBoostClassifier(**best_params_adaboost, base_estimator=DecisionTreeClassifier(), random_state=42)),
        ('xgb', XGBClassifier(**best_params_xgboost, random_state=42))
    ],
    final_estimator=final_estimator_rf
)

# Fit the final stacking model on the scaled training data
final_stacking_model.fit(X_train_scaled, y_train)

y_pred_final_stacking = final_stacking_model.predict(X_test_scaled)

# Calculate metrics and print out report for Stacking model
accuracy_stacking = accuracy_score(y_test, y_pred_final_stacking)
print(f'Accuracy for Stacking Model: {accuracy_stacking:.2f}')
print(classification_report(y_test, y_pred_final_stacking))

# Predict probabilities for the positive class (class 1)
y_prob_stacking = final_stacking_model.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and ROC area for each class
fpr_stacking, tpr_stacking, _ = roc_curve(y_test, y_prob_stacking)
roc_auc_stacking = auc(fpr_stacking, tpr_stacking)

# Plot ROC curve for Stacking model
plt.figure(figsize=(8, 6))
plt.plot(fpr_stacking, tpr_stacking, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_stacking:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Stacking Model ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Print the best parameters for each model and the stacking model
print("\nBest Parameters for AdaBoost:")
print(best_params_adaboost)
print("\nBest Parameters for GradientBoost:")
print(best_params_gradientboost)
print("\nBest Parameters for XGBoost:")
print(best_params_xgboost)
print("\nBest Parameters for Stacking Model:")
print(best_params_stacking)